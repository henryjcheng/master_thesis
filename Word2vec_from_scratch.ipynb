{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec from Scratch with Python and NumPy\n",
    "From article:  \n",
    "https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal with word2vec and most NLP embedding schemes is to translate text into vectors so that they can then be processed using operations from linear algebra.  \n",
    "Vectorizing text data allows us to then create predictive models that use these vectors as input to perform something useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW\n",
    "Given a word in a sentence, w(t) (aka _center word_ or _target word_), CBOW uses the context or surrounding words as input and tries to predict the target word.\n",
    "\n",
    "#### Skip-gram\n",
    "skip-gram use a center word to predict the context words.\n",
    "\n",
    "__Skip-gram__ has been shown to produce bettwer word-embeddings than __CBOW__.\n",
    "\n",
    "#### one-hot encoding\n",
    "Because we can't send text data directly through a matrix, we need to employ _one-hot encoding_.  \n",
    "This means we have a vector of length _v_ where v is the total number of unique words in the text corpus. Each word corresponds to a single position in this vector, so when embedding the word v_n, everywhere in vector v is zero except v_n, which equals 1. \n",
    "\n",
    "After _one-hot encoding_, we can feed the data into network and train it.  \n",
    "Network archetecture:  \n",
    "\n",
    "Input Layer(Vx1) x W1(VxN) x Hidden Layer(Nx1) x W1'(NxV) x Output Layer(CxV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = settings['n']\n",
    "        self.eta = settings['learning_rate']\n",
    "        self.epochs = settings['epochs']\n",
    "        self.window = settings['window_size']\n",
    "        pass\n",
    "    \n",
    "    # generate training data\n",
    "    def generate_training_data(self, settings, corpus):\n",
    "        \n",
    "        # generate word counts\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "        \n",
    "        self.v_count = len(word_counts.keys())\n",
    "        \n",
    "        # generate lookup dictionaries\n",
    "        self.words_list = sorted(list(word_counts.keys()), reverse=False)\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        # cycle through each sentence in corpus\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "            \n",
    "            # cycle through each word in sentence\n",
    "            for i, word in enumerate(sentence):\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "                \n",
    "                # cycle through context window\n",
    "                w_context = []\n",
    "                for j in range(i-self.window, i+self.window+1):\n",
    "                    if j != i and j <= sent_len-1 and j >= 0:\n",
    "                        w_context.append(self.word2onehot(sentence[j]))\n",
    "                training_data.append([w_target, w_context])\n",
    "        return np.array(training_data)\n",
    "    \n",
    "    # convert word to one-hot encoding\n",
    "    def word2onehot(self, word):\n",
    "        word_vec = [0 for i in range(0, self.v_count)]\n",
    "        word_index = self.word_index[word]\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "    \n",
    "    # forward pass\n",
    "    def forward_pass(self, x):\n",
    "        h   = np.dot(self.w1.T, x)\n",
    "        u = np.dot(self.w2.T, h)\n",
    "        y_c = self.softmax(u)\n",
    "        return y_c, h, u\n",
    "\n",
    "    # softmax activation function\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    # train w2v model\n",
    "    def train(self, training_data):\n",
    "        # initialize weight matrices\n",
    "        self.w1 = np.random.uniform(-.8, .8, (self.v_count, self.n))  # context matrix\n",
    "        self.w2 = np.random.uniform(-.8, .8, (self.n, self.v_count))  # embedding matrix\n",
    "\n",
    "        # cycle through each epoch\n",
    "        for i in range(0, self.epochs):\n",
    "\n",
    "            self.loss = 0\n",
    "\n",
    "            # cycle through each training sample\n",
    "            for w_t, w_c in training_data:\n",
    "                y_pred, h, u = self.forward_pass(w_t)                            # forward pass\n",
    "                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0) # calculate error\n",
    "                self.backprop(EI, h, w_t)                                        # backpropagation\n",
    "\n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c)*np.log(np.sum(np.exp(u)))\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print('EPOCH: ' + str(i) + ' LOSS: ' + str(self.loss))\n",
    "        pass\n",
    "\n",
    "    # backpropagation\n",
    "    def backprop(self, e, h, x):\n",
    "        d1_dw2 = np.outer(h, e)\n",
    "        d1_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "\n",
    "        # update weights\n",
    "        self.w1 = self.w1 - (self.eta * d1_dw1)\n",
    "        self.w2 = self.w2 - (self.eta * d1_dw2)\n",
    "    \n",
    "    # input a word, returns a vector (if available)\n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_index[word]\n",
    "        v_w     = self.w1[w_index]\n",
    "        return v_w\n",
    "    \n",
    "    \n",
    "    # input a vector, returns nearest word(s)\n",
    "    def vec_sim(self, vec, top_n):\n",
    "\n",
    "        # CYCLE THROUGH VOCAB\n",
    "        word_sim = {}\n",
    "        for i in range(self.v_count):\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_num = np.dot(vec, v_w2)\n",
    "            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2)\n",
    "            theta = theta_num / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda sim:(word, sim), reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print (word, sim)\n",
    "            \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 LOSS: 68.37096376709991\n",
      "EPOCH: 1000 LOSS: 41.24645176265884\n",
      "EPOCH: 2000 LOSS: 41.13428630385451\n",
      "EPOCH: 3000 LOSS: 41.10145846484696\n",
      "EPOCH: 4000 LOSS: 41.080509291741805\n"
     ]
    }
   ],
   "source": [
    "settings = {}\n",
    "settings['n'] = 5\n",
    "settings['window_size'] = 2\n",
    "settings['min_count'] = 0\n",
    "settings['epochs'] = 5000\n",
    "settings['neg_samp'] = 10\n",
    "settings['learning_rate'] = .01\n",
    "np.random.seed(0)\n",
    "\n",
    "corpus = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "\n",
    "# initialize w2v model\n",
    "w2v = word2vec()\n",
    "\n",
    "training_data = w2v.generate_training_data(settings, corpus)\n",
    "\n",
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-6e4dac7ddffb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fox'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-ecfca8311a18>\u001b[0m in \u001b[0;36mvec_sim\u001b[1;34m(self, vec, top_n)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mv_w2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mtheta_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_w2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mtheta_den\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_w2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta_num\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtheta_den\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('<U32') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "w2v.vec_sim('fox', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
